{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b21ade6",
   "metadata": {},
   "source": [
    "# Deep Learning for Femur Segmentation\n",
    "\n",
    "This is a personal project to create a UNet model that can segment fluoroscopy images of femurs. Input and output images have been omitted from display as the dataset used is not public. <br>\n",
    "\n",
    "Code in this notebook is referenced from the following repo and also assistance from others: https://github.com/Mostafa-wael/U-Net-in-PyTorch/blob/main/U_Net.ipynb \n",
    "\n",
    "Changes made from example code above include:\n",
    "1) Modifications to work with custom femur dataset <br>\n",
    "2) Addition of train loss plot <br>\n",
    "3) Implementation of metrics <br> \n",
    "4) Export to ONNX and mean inference time <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c68c3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Io\\anaconda3\\envs\\env_torch\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\Io\\anaconda3\\envs\\env_torch\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "C:\\Users\\Io\\anaconda3\\envs\\env_torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from functools import reduce\n",
    "import itertools\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import glob\n",
    "import cv2\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "from byol_pytorch import BYOL\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be27e9e8",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "These helper functions are used to create the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e8b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_crop(img):\n",
    "    # crops an image to have its height match its width.\n",
    "    # crop is done only on the larger dim.\n",
    "\n",
    "    h = None\n",
    "    w = None\n",
    "    \n",
    "    if len(img.shape) == 3:\n",
    "        h, w, _ = img.shape\n",
    "    else:\n",
    "        h, w = img.shape\n",
    "    \n",
    "    if h > w:\n",
    "        diff = int(h - w) // 2\n",
    "        img = img[diff:h-diff:, :]\n",
    "    else:\n",
    "        diff = int(w - h) // 2\n",
    "        img = img[:, diff:w-diff]\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_image(image_path, mode, image_size, threshold=False):\n",
    "    # thresholding fixes JPG artifacts\n",
    "    img = cv2.imread(image_path, mode)\n",
    "    img = square_crop(img)\n",
    "    if threshold:\n",
    "        _, img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "    if image_size != -1:\n",
    "        img = cv2.resize(img, (image_size, image_size))\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edf3a5a",
   "metadata": {},
   "source": [
    "### Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25d0d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FemurDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_size=-1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.img_size  = image_size\n",
    "        self.transform = transforms.RandomInvert(p=0.05)\n",
    "\n",
    "        # load paths from pickle files\n",
    "        image_paths = []\n",
    "        label_paths = []\n",
    "\n",
    "        image_paths = glob.glob(r'C:\\dramadas\\data\\femur_seg_images\\*', recursive=True)\n",
    "        label_paths = glob.glob(r'C:\\dramadas\\data\\femur_seg_masks\\*', recursive=True)\n",
    "\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "        assert len(self.image_paths) == len(self.label_paths)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    " \n",
    "        image     = load_image(self.image_paths[idx], cv2.IMREAD_COLOR,     self.img_size, False)\n",
    "\n",
    "        gt_mask   = load_image(self.label_paths[idx], cv2.IMREAD_GRAYSCALE, self.img_size, True)\n",
    " \n",
    "        image_t   = torch.Tensor(image/(255.0)).permute(2,0,1)\n",
    "\n",
    "        gt_mask_t = torch.Tensor(gt_mask/ 255.0).unsqueeze(0)\n",
    "\n",
    " \n",
    "        return [image_t, gt_mask_t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a620757d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size:         torch.Size([3, 256, 256])\n",
      "GT Mask size:       torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "ds = FemurDataset(\n",
    "    image_size=256,\n",
    ")\n",
    "\n",
    "image, gt_mask = ds[0]\n",
    "\n",
    "print('Image size:        ',  image.size())\n",
    "print('GT Mask size:      ',  gt_mask.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c0a17e",
   "metadata": {},
   "source": [
    "Note: Check for mins and max to see if normalization is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a758ba81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fbf5a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde263e",
   "metadata": {},
   "source": [
    "## View images here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5680c332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256, 3])\n"
     ]
    }
   ],
   "source": [
    "image = image.permute(1,2,0)\n",
    "print(image.shape)\n",
    "# plt.imshow(image)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c14ab2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = gt_mask.squeeze()\n",
    "# plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c03e76",
   "metadata": {},
   "source": [
    "### Dataloader creation\n",
    "\n",
    "The train/test split used was 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64aeb591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1,053\n",
      "Test dataset size:  116\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "# split into train/test\n",
    "test_size  = int(len(ds) * 0.1)\n",
    "train_size = len(ds) - test_size\n",
    "print('Train dataset size: {:,}'.format(train_size))\n",
    "print('Test dataset size:  {:,}'.format(test_size))\n",
    "train_ds, test_ds = torch.utils.data.random_split(ds, [train_size, test_size])\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_ds,  batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc330f",
   "metadata": {},
   "source": [
    "## Model Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc09440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, num_epochs=5):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    best_loss = 1e10\n",
    "    loss_fn    = torch.nn.BCEWithLogitsLoss()\n",
    "    train_loss = 0.0\n",
    "    all_train_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(\"LR\", param_group['lr'])\n",
    "\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        metrics = defaultdict(float)\n",
    "        epoch_samples = 0\n",
    "\n",
    "        for i, (image, gt_mask) in enumerate(tqdm(train_loader)):\n",
    "            inputs = image.to(device)\n",
    "            labels = gt_mask.to(device)\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs.to(device), labels)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # backward + optimize only if in training phase\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            epoch_samples += inputs.size(0)\n",
    "\n",
    "            #print_metrics(metrics, epoch_samples)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "            \n",
    "            # update train loss\n",
    "            train_loss = train_loss + ((1 / (i + 1)) * (loss - train_loss))\n",
    "\n",
    "        train_loss = train_loss / len(train_loader) \n",
    "        print('Train loss: {:,}'.format(train_loss))\n",
    "        all_train_losses.append(train_loss.item())\n",
    "\n",
    "    plt.plot(all_train_losses)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e92c80",
   "metadata": {},
   "source": [
    "## UNet Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c157054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the necessary modules are imported from the torch and torchvision packages, including the nn module for building neural networks and the pre-trained models provided in torchvision.models. \n",
    "# The relu function is also imported from torch.nn.functional.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "\n",
    "# Then, a custom class UNet is defined as a subclass of nn.Module. \n",
    "# The __init__ method initializes the architecture of the U-Net by defining the layers for both the encoder and decoder parts of the network. \n",
    "# The argument n_class specifies the number of classes for the segmentation task.\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1) \n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) \n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) \n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        # In the decoder, transpose convolutional layers with the ConvTranspose2d function are used to upsample the feature maps to the original size of the input image. \n",
    "        # Each block in the decoder consists of an upsampling layer, a concatenation with the corresponding encoder feature map, and two convolutional layers.\n",
    "        # -------\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "        \n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3785ae3d-bc30-49dd-8f21-aa5cad070e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6405e2d",
   "metadata": {},
   "source": [
    "### Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47dc9ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:14<00:00, 37.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.000805570394732058\n",
      "Epoch 1/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0006636857870034873\n",
      "Epoch 2/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0005407611606642604\n",
      "Epoch 3/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00040144388913176954\n",
      "Epoch 4/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0002845249546226114\n",
      "Epoch 5/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00021727022249251604\n",
      "Epoch 6/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00016706286987755448\n",
      "Epoch 7/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00012001071445411071\n",
      "Epoch 8/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00011007461580447853\n",
      "Epoch 9/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 7.555248885182664e-05\n",
      "Epoch 10/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 8.314515434904024e-05\n",
      "Epoch 11/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 6.827019387856126e-05\n",
      "Epoch 12/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 5.670906466548331e-05\n",
      "Epoch 13/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 7.568490400444716e-05\n",
      "Epoch 14/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.55875342595391e-05\n",
      "Epoch 15/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8926478737266734e-05\n",
      "Epoch 16/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.682324677356519e-05\n",
      "Epoch 17/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 5.768265327787958e-05\n",
      "Epoch 18/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 5.9928453993052244e-05\n",
      "Epoch 19/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.436277256696485e-05\n",
      "Epoch 20/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.274718594388105e-05\n",
      "Epoch 21/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.630372884799726e-05\n",
      "Epoch 22/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.046568963327445e-05\n",
      "Epoch 23/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9794704207452014e-05\n",
      "Epoch 24/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.320457446738146e-05\n",
      "Epoch 25/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.639856029418297e-05\n",
      "Epoch 26/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.651043178047985e-05\n",
      "Epoch 27/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.6204954085405916e-05\n",
      "Epoch 28/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 43.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.6404584787087515e-05\n",
      "Epoch 29/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 45.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.0861984012299217e-05\n",
      "Epoch 30/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.9780136426561512e-05\n",
      "Epoch 31/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.8663144146557897e-05\n",
      "Epoch 32/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.8136845028493553e-05\n",
      "Epoch 33/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.825170693336986e-05\n",
      "Epoch 34/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.71872192772571e-05\n",
      "Epoch 35/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.301784429117106e-05\n",
      "Epoch 36/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.054683475056663e-05\n",
      "Epoch 37/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.963441536645405e-05\n",
      "Epoch 38/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.56544065248454e-05\n",
      "Epoch 39/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.4488558917946648e-05\n",
      "Epoch 40/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.4409634786716197e-05\n",
      "Epoch 41/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.4592305888072588e-05\n",
      "Epoch 42/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.4719992577738594e-05\n",
      "Epoch 43/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.4556893802364357e-05\n",
      "Epoch 44/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.4544460100296419e-05\n",
      "Epoch 45/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.626567169907503e-05\n",
      "Epoch 46/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.8133335945312865e-05\n",
      "Epoch 47/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.4556791029463056e-05\n",
      "Epoch 48/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2657671504712198e-05\n",
      "Epoch 49/49\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:11<00:00, 44.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1976357200182974e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_class = 1 \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNet(num_class).to(device)\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "model = train_model(model, optimizer_ft, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64b9b3d-90d5-4f2c-a83c-eb57a7caae87",
   "metadata": {},
   "source": [
    "### Save model checkpoint here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7385c34e-5f0d-4092-88fb-8e763c98ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model here\n",
    "import torch\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58f68319-eba0-4a0a-9b14-80eb9104bee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef403e7d-076f-483c-8fc2-2749860bd964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (e11): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e21): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e31): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e32): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e41): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e51): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e52): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (d11): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (d12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (d21): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (d22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (d31): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (d32): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (d41): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (d42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (outconv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the model here\n",
    "model = UNet(1) # we do not specify ``weights``, i.e. create untrained model\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772eb4e0",
   "metadata": {},
   "source": [
    "### Evaluate model for 1 test image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "911e196f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape torch.Size([1, 3, 256, 256])\n",
      "labels shape torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set model to the evaluation mode\n",
    "\n",
    "test_loader  = torch.utils.data.DataLoader(test_ds,  batch_size=1, shuffle=True) # this line is repeated\n",
    "\n",
    "# Get the first batch\n",
    "inputs, labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "print(\"inputs shape\", inputs.shape)\n",
    "print(\"labels shape\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b92e618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted mask shape torch.Size([1, 1, 256, 256])\n",
      "(1, 1, 256, 256)\n",
      "inputs shape torch.Size([3, 256, 256])\n",
      "Labels shape (1, 1, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "pred = model(inputs)\n",
    "# pred = pred.to(device)\n",
    "# The loss functions include the sigmoid function.\n",
    "pred = F.sigmoid(pred)\n",
    "pred_seg = pred.data.cpu().numpy()\n",
    "print(\"predicted mask shape\", pred.shape)\n",
    "pred_seg = (pred_seg > 0.5).astype(np.uint8)\n",
    "print(pred_seg.shape)\n",
    "inputs = inputs.squeeze(0)\n",
    "print(\"inputs shape\", inputs.shape)\n",
    "print(\"Labels shape\", labels.cpu().numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8de5bb",
   "metadata": {},
   "source": [
    "### Metrics For Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52ce8182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import BinaryAccuracy\n",
    "from torchmetrics.classification import BinaryPrecision\n",
    "from torchmetrics.classification import BinaryRecall\n",
    "from torchmetrics.classification import BinaryF1Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc91cef",
   "metadata": {},
   "source": [
    "Metrics from here: https://lightning.ai/docs/torchmetrics/stable/classification/accuracy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9bb4523",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (labels>0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "753ad1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9891, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric1 = BinaryAccuracy().to(device)\n",
    "metric1(pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b5c4234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9882, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric2 = BinaryPrecision().to(device)\n",
    "metric2(pred,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51cdb2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9654, device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric3 = BinaryRecall().to(device)\n",
    "metric3(pred,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "caacf13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9767, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric4 = BinaryF1Score().to(device)\n",
    "metric4(pred,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee00875",
   "metadata": {},
   "source": [
    "## View Test Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54087240",
   "metadata": {},
   "source": [
    "### Original mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01f42cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "labels = labels.cpu()\n",
    "labels = labels.squeeze()\n",
    "print(labels.shape)\n",
    "\n",
    "# plt.imshow(labels)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043fe461",
   "metadata": {},
   "source": [
    "### Model Generated Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfd26fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256)\n"
     ]
    }
   ],
   "source": [
    "pred_seg = pred_seg.squeeze()\n",
    "print(pred_seg.shape)\n",
    "\n",
    "# plt.imshow(pred_seg)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf5e2a",
   "metadata": {},
   "source": [
    "### Original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fe3d220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256, 3])\n"
     ]
    }
   ],
   "source": [
    "inputs = inputs.cpu()\n",
    "#image = inputs/(255.0)\n",
    "image = inputs.permute(1,2,0)\n",
    "print(image.shape)\n",
    "\n",
    "# plt.imshow(image)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378a70b1",
   "metadata": {},
   "source": [
    "### Image with mask overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a1423",
   "metadata": {},
   "source": [
    "Code taken from here to do the overlays: https://stackoverflow.com/questions/10127284/overlay-imshow-plots-in-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40518a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import colorConverter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "# create dummy data\n",
    "inputs = inputs.cpu()\n",
    "#image = inputs/(255.0)\n",
    "image = inputs.permute(1,2,0)\n",
    "zvals = image\n",
    "zvals2 = pred_seg\n",
    "\n",
    "# generate the colors for your colormap\n",
    "color1 = colorConverter.to_rgba('black')\n",
    "color2 = colorConverter.to_rgba('yellow')\n",
    "\n",
    "# make the colormaps\n",
    "cmap1 = mpl.colors.LinearSegmentedColormap.from_list('my_cmap',['red','green'],256)\n",
    "cmap2 = mpl.colors.LinearSegmentedColormap.from_list('my_cmap2',[color1,color2],256)\n",
    "\n",
    "cmap2._init() # create the _lut array, with rgba values\n",
    "\n",
    "# create your alpha array and fill the colormap with them.\n",
    "# here it is progressive, but you can create whathever you want\n",
    "alphas = np.linspace(0, 0.8, cmap2.N+3)\n",
    "cmap2._lut[:,-1] = alphas\n",
    "\n",
    "# img2 = plt.imshow(zvals, interpolation='nearest', cmap=cmap1, origin='upper')\n",
    "# img3 = plt.imshow(zvals2, interpolation='nearest', cmap=cmap2, origin='upper')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8478f2a",
   "metadata": {},
   "source": [
    "## Evaluate model for all test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d34474df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [00:02<00:00, 42.56it/s]\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1scores = []\n",
    "for i, (image, gt_mask) in enumerate(tqdm(test_loader)):\n",
    "    inputs = image.to(device)\n",
    "    labels = gt_mask.to(device)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    #Binarize target mask\n",
    "    labels = (labels>0.5).float()\n",
    "\n",
    "    #Accuracy\n",
    "    metric1 = BinaryAccuracy().to(device)\n",
    "    acc = metric1(outputs, labels)\n",
    "    accuracies.append(acc.item())\n",
    "\n",
    "    \n",
    "    #Precision\n",
    "    metric2 = BinaryPrecision().to(device)\n",
    "    prec = metric2(outputs, labels)\n",
    "    precisions.append(prec.item())\n",
    "    \n",
    "    #Recall\n",
    "    metric3 = BinaryRecall().to(device)\n",
    "    rec = metric3(outputs, labels)\n",
    "    recalls.append(rec.item())\n",
    "\n",
    "    #F1\n",
    "    metric4 = BinaryF1Score().to(device)\n",
    "    f1_s = metric4(outputs, labels)\n",
    "    f1scores.append(f1_s.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a67c845b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.990734758048222\n",
      "0.9817432252497509\n",
      "0.96905434080239\n",
      "0.9738472926205602\n"
     ]
    }
   ],
   "source": [
    "print(np.average(accuracies))\n",
    "print(np.average(precisions))\n",
    "print(np.average(recalls))\n",
    "print(np.average(f1scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be48a0f6",
   "metadata": {},
   "source": [
    "### To see summary/parameters of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59170cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_model_summary as pms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64f541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = UNet(2)\n",
    "print(pms.summary(\n",
    "            model,\n",
    "            torch.zeros((8, 3, 256, 256)),\n",
    "            show_input=True\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242bfd42",
   "metadata": {},
   "source": [
    "## Exporting to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a772d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard imports\n",
    "from torch import nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx\n",
    "import onnxruntime\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51609b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, 3, 256, 256, requires_grad=True).to(device)\n",
    "torch_out = model(x)\n",
    "torch.onnx.export(model, \n",
    "                  x, \n",
    "                  \"UNet.onnx\",\n",
    "                 export_params = True,\n",
    "                 opset_version=10,\n",
    "                 do_constant_folding=True,\n",
    "                 input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38d4e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"UNet.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771944af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = onnxruntime.InferenceSession(\"UNet.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "for i in range(1000):\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "end_time = time.perf_counter()\n",
    "print(\"Mean inference time\", (end_time - start_time)/1000)\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb76365",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean inference time\", (end_time - start_time)/1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
